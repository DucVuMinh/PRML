{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability vs Probability Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As opposed to PMF, PDF ***can*** be > 1, and does ***not*** give point probability $\\text{Prob}\\left(\\mathbf{X}=\\mathbf{x}_{0}\\right)$ -> hence the need for normalized distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian learning (binary)\n",
    "* *Keywords: Bernoulli, Binomial, Beta distrib.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Tossing a coin $N=3$ times. Observed number of heads $m=3$ and number of tails $l=0$. Predict outcome of next tosses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**: Each toss is a <font color='blue'>Bernoulli</font> event $$p\\left(t|\\Theta\\right)=\\text{Bernoulli}\\left(t|\\mu\\right)=\\mu^{t}\\left(1-\\mu\\right)^{1-t}=\\begin{cases}\n",
    "\\mu & \\text{if }t=1\\\\\n",
    "1-\\mu & \\text{if }t=0\n",
    "\\end{cases}$$ where $t\\in\\left\\{ 0,1\\right\\} $ and $\\Theta=\\left\\{ \\mu\\right\\}$ .\n",
    " Therefore $N$ (independent) tosses gives <font color='blue'>**Likelihood**</font> $$p\\left(\\mathcal{D}|\\Theta\\right)=\\prod_{n}^{N}\\mu^{t_{n}}\\left(1-\\mu\\right)^{1-t_{n}}\n",
    " $$\n",
    "where $\\mathcal{D}=\\left\\{ t_{1..N}\\right\\}$. Predict outcome of next tosses $\\equiv$ ***Estimate $\\mu$***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>*Note*</font>: the above likelihood can be written as \n",
    "$p\\left(\\mathcal{D}|\\Theta\\right)=\\mu^{m}\\left(1-\\mu\\right)^{N-m}$ which is the unnormalized pmf of a <font color='blue'>Binomial</font> distribution. In fact, Bernoulli is a *special case* of the Binomial. Additionally, the problem can be modelled using the Binomial directly via an *alternative problem*.\n",
    "> Throw $N$ coins which have the same fairness $\\mu$ to the ground. Observed $m$ heads and $l$ tails. Predict number of heads if throwing other $N'$ coins which also have the same fairness $\\mu$.\n",
    "\n",
    "In this case, \n",
    "$$p\\left(\\mathcal{D}|\\Theta\\right)=\\left(\\begin{array}{c}\n",
    "N\\\\\n",
    "m\n",
    "\\end{array}\\right)\\mu^{m}\\left(1-\\mu\\right)^{N-m}$$\n",
    "where $\\mathcal{D}=\\left\\{ m\\right\\}$ and $\\Theta=\\left\\{ N,\\mu\\right\\} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference and Predictive distribution**:\n",
    "* Frequentist: \n",
    "    * <font color='blue'>**Parameter** estimate</font>: \n",
    "    $$\\hat{\\mu}_\\text{MLE}=\\dfrac{m}{N}=1$$\n",
    "    * <font color='blue'>**Predictive distribution**</font>: \n",
    "    $$p\\left(t^{\\left(\\text{new}\\right)}=1|\\mathcal{D}\\right)=\\text{Bernoulli}\\left(t^{\\left(\\text{new}\\right)}=1|\\mu=1\\right)=1$$ i.e. all future tosses will be heads (!)\n",
    "* Bayesian: Hm.. Such extreme estimate based on such limited observation by MLE. It seems more intuitive to \n",
    "    * formally measure *uncertainty $p\\left(\\mu|\\cdot\\right)$ of the parameter $\\mu$*, and\n",
    "    * *updating prediction* upon new tosses **sequentially** when new data come."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Parameter Prior**</font>: $$p\\left(\\mu\\right|\\mathcal{H})=\\text{Beta}\\left(\\mu|a,b\\right)=\\dfrac{\\Gamma\\left(a+b\\right)}{\\Gamma\\left(a\\right)\\Gamma\\left(b\\right)}\\mu^{a-1}\\left(1-\\mu\\right)^{b-1}$$ for ***conjugacy***. Hyperparameters $\\mathcal{H}=\\left\\{ a,b\\right\\} $ are interpreted as *effective* (prior and observed) number of heads and tails respectively (note *effective*, as a posterior can be used as a prior in ***sequential learning*** framework)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>*Note*</font>: $\\Gamma\\left(z+1\\right)=z\\cdot\\Gamma\\left(z\\right)$ for $z\\in\\mathbb{R}\n",
    " $, and $\\Gamma\\left(n+1\\right)=n!$ for $n\\in\\mathbb{N}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Parameter Posterior**</font>: $$p\\left(\\mu|\\mathcal{D},\\mathcal{H}\\right)=\\text{Beta}\\left(a+m-1,b+l-1\\right)$$\n",
    "<img src=\"figs/beta.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Predicitve distribution**</font>: $$p\\left(t^{\\left(\\text{new}\\right)}=1|\\mathcal{D},\\mathcal{H}\\right)=\\int_{\\mu}p\\left(t=1|\\mathcal{\\mu},\\mathcal{D},\\mathcal{H}\\right)p\\left(\\mu|\\mathcal{D},\\mathcal{H}\\right)\\text{d}\\mu=\\dfrac{m+a}{N+a+b}$$\n",
    "In concrete, $p\\left(t^{\\left(\\text{new}\\right)}=1|\\mathcal{D},\\mathcal{H}\\right)=...=\\int_{\\mu}\\mu p\\left(\\mu|\\mathcal{D},\\mathcal{H}\\right)\\text{d}\\mu=\\mathbb{E}_{\\mu}\\left[\\mu|\\mathcal{D},\\mathcal{H}\\right]=\\dfrac{m+a}{N+a+b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian learning (categorical)\n",
    "* *Keywords: Categorical, Multinomial, Dirichlet distrib. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Rolling dice. There are $K=6$ possible outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**: Each roll is a <font color='blue'>Categorical</font> (or generalized Bernoulli) event\n",
    "$$p\\left(\\mathbf{t}|\\Theta\\right)=\\prod_{k}\\mu_{k}^{t_{k}}$$\n",
    "where $\\mathbf{t}$ is 1-of-K encoding, $\\Theta=\\left\\{\\mu_{1..K}\\right\\} $ and $\\sum_{k}\\mu_{k}=1$. \n",
    " \n",
    " Therefore $N=\\sum_{k}m_{k}$ (independent) rolls give <font color='blue'>**Likelihood**</font> \n",
    " $$p\\left(\\mathcal{D}|\\Theta\\right)=\\prod_{n}^{N}\\prod_{k}^{K}\\mu_{k}^{t_{nk}}\n",
    " $$\n",
    " where $\\mathcal{D}=\\left\\{t_{1..N,1..K}\\right\\} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>*Note*</font>: the above likelihood can be written as \n",
    "$p\\left(\\mathcal{D}|\\Theta\\right)=\\prod_{k}^{K}\\mu_{k}^{m_{k}}\n",
    "$ which is the unnormalized pmf of a <font color='blue'>Multinomial</font> distribution. As above, Categorical is a *special case* of the Multinomial, and the original problem can be modelled using the Multinomial indirectly via an *alternative problem*. In this case, \n",
    "$$p\\left(\\mathcal{D}|\\Theta\\right)=\\left(\\begin{array}{c}\n",
    "N\\\\\n",
    "m_{1}m_{2}...m_{K}\n",
    "\\end{array}\\right)\\prod_{k}^{K}\\mu_{k}^{m_{k}}$$\n",
    "where $\\mathcal{D}=\\left\\{m_{1..K}\\right\\}$ and $\\Theta=\\left\\{ N,\\mu_{1..K}\\right\\} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Parameter Prior**</font>: $$p\\left(\\mathbf{\\mu}\\right|\\mathcal{H})=\\text{Dir}\\left(\\mathbf{\\mu|\\alpha}\\right)=\\dfrac{\\Gamma\\left(\\sum_{k}\\alpha_{k}\\right)}{\\prod_{k}\\Gamma\\left(\\alpha_{k}\\right)}\\prod_{k}\\mu^{\\alpha_{k}-1}$$ for *conjugacy*. Dirichlet hyperparameters $\\mathcal{H}=\\left\\{\\alpha_{1..K}\\right\\} $ are interpreted as *effective* (prior and observed) number of each outcome respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Parameter Posterior**</font>: $$p\\left(\\mathbf{\\mu}|\\mathcal{D},\\mathcal{H}\\right)=\\text{Dir}\\left(\\mathbf{\\alpha+m}\\right)$$\n",
    "<img src=\"figs/dirichlet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian learning (continuous)\n",
    "*Keywords:*\n",
    "* (univariate) *Gaussian, Gamma, Gaussian-Gamma distrib.* \n",
    "* (multivariate) *Gaussian, Wishart, Gaussian-Wishart distrib. *"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
